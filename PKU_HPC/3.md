## CPU and GPU

- 什么是 profiling
- cpu 上的 profiling
- gpu 上的

什么是 profiling

- Amdahl Law：优化热点 hotspot 的收益明显更大
- $\frac{1}{(1 - p) + \frac{p}{s}}$ p 是 优化部分占比，s 是优化倍率
- profiling 就是找出热点的过程
- 宏观层面，找出程序中哪个部分耗时最长
- 获取某个函数运行时的 metric，以优化具体的函数（微观层面）

什么是 profiler

- 完成 profiling 的辅助工具叫 profiler
- 可能会间歇性的打断程序运行
- 与编译器合作，在函数的入口统计调用次数、运行总时长等
- 部分硬件具有 metric 统计功能，如缓存命中率、内存带宽使用率、occupancy 等
- 也可以不用工具，在每个函数的前后分别调用 time 来计算耗时

cpu 的 profiler gpu

- gprof，gmon.out，flat profile 自身耗时，cat profile 调用关系
- O1 和 O2 的优化，对 stl 非常敏感
- 在编译连接的时候，可以用 gprof 来解析对应的文件
- 可以查找 gnu gprof 关键词来搜索用法
- flame graph（是由 Linux 性能优化大师 Brendan Gregg 发明的用于分析性能瓶颈的可视化图表，它以一个全局的视野来看待时间分布）

Intel VTune

- Intel VTune 是 Intel 官方的 oneAPI 套件库中的 Profiler，可以用在 Intel CPU 上进行非常全面的性能分析（hotspot、并行情况、通信容量、微架构执行详情）
- AMD、Intel、Nvidia
- 并行时比较重要的提升，因为 gprof 发明的时候并没有并行、超算、集群这种概念
- vtune 整个流程非常专业，非常全面；可以面向超算等

GPU 上的 Profiling

- 做 ai 的，需要判断性能瓶颈是位于cpu还是gpu，并考虑 cpu 和 gpu 之间的同步开销（用 pytorch）
- 如果是 gpu kernel 的编写者，那么可能需要对某个 kernel 进行 profile，以找到 kernel 的潜在优化点（写 pytorch）
- cuda 编程教程
  - https://www.bilibili.com/video/BV1424yli7xe
  - Https://wiki.1cpu.dev/zh/hpc/from-scratch/cuda

cpu 和 gpu 执行：

- gpu 协处理器，正常来说 gpu 跑什么是 cpu 说了算
- cpu 会向 gpu 发射 kernel（gpu 上执行的函数）
- cpu sync（阻塞，等待 gpu 执行完毕）
- 发射 kernel 会有一些 overhead（消耗时间）

以一个 mnist 训练代码为例子，用 nvidia nsight system 来进行监控。在训练结束之后，会有一个nsys-rep 的文件，打开之后，会有一些 flame graph

- 大部分时间 gpu 都是空闲的
- sem_timewait 很长（这个是 cpu 在干活 gpu 在空闲）
- cpu 在真正的高性能框架下，cpu 会发送很多 kernel
  - cpu 快于 gpu，gpu 则会打满
  - cpu 同步太多，gpu 会空载
  - cpu 慢于 gpu，gpu 也会空
  - 在 ai 领域 cpu会快于 gpu，cpu同步不太多
- mnist 的这个代码慢在 dataloader 里面，没有开 num_workers 这个选项，一旦选择了 num_workers = 4，则比原来快了3倍
- 可不可以做的更好？
  - 可以，对于小数据集来说，可以把全部数据都放在 gpu 显存里面
  - 这样数据同步就不需要时间了
  - kernel 比较小，可以试试看开大batch size（训练集）
  - 如果显存不够大（A100 最大可以开 80g），可以考虑开启 torch.DataLoader 的 pin_memory 模式，执行异步内存拷贝
  - 某些 kernel 执行时间很短，但调用次数很多，导致很大的 kernel 启动开销 or 重复访问：使用 torch.jit 进行 kernel fusion（layernorm，或者GeLU之类的）

对单个 kernel 进行 profile

- 以 transpose kernel 为例，演示 ksight compute

~~~c++
__global__ void transpose_kernel_slow(
	T* __restrict__ output,
  T* __restrict__ input,
  int N, int M
) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < M and j < N) {
    output[i * N + j] * input[j * M + i];
  }
}// 问题就是访存不连续

constexpr int TILE_SIZE = 32;
__global__ void transpose_kernel_fast(
	T* __restrict__ output,
  T* __restrict__ input,
  int N, int M
) {
  __shared__ T tile[TILE_SISZE][TILE_SIZE];
  int i = blockIdx.x * TILE_SIZE + threadIdx.x;
  int j = blockIdx.y * TILE_SIZE + threadIdx.y;
  tile[threadIdx.y][threadIdx.x] = input[j * M + i];
  __syncthreads();
  i = blockIdx.y * TILE_SIZE + threadIdx.x;
  j = blockIdx.x * TILE_SIZE + threadIDX.y;
  output[j * N + i] = tile[threadIdx.x][threadIdx.y];
}// 
~~~



对单个kernel 进行 profile，nsight compute 还能提供许许多多的 metric

- 计算单元、内存总线利用率
- roofline model
- 各种指令的条数
- 运算单元活跃度
- instruction scheduler 统计信息
- 等等















































