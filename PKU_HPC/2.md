# GPU Programming

~~~c++
__global__ void vecADD(double* a, double* b, double* c, int n) {
  int id = blockIdx.x * blockDim.x + threadIdx.x;
  if (id < n)
    c[id] = a[id] + b[id];
}
~~~

Why is gpu so fast

- gpu 的设计是并行掩盖访存延迟，cpu 的多层 cach 设计导致访存延迟被提高很多。gpu 就是一次性搬运很多内存

每个 block 会匹配一个 streaming multiprocessor 上，每个 sm 里面会有很多 cuda core

- A100 为例子，sm 有 16 * 4 = 64 个 cuda core 
- gpu 上下文切换，gpu有很多寄存器，做切换很快

gpu compute architecture（sm里面有block，有core，core里面有很多thread）

gpu memory architecture

- A100为例子，40g 显存，40MB L2
- 显存到 L2 1.55TB/s，memory through put 很变态
- memory coalescing，gpu 里面是按照 batch来干活的，交错访存的模式
- banks 和 shared memory：bank 是 每个 block controllable
- bank conflict：bank 在一个使用周期是只能被一个 thread 访问的，所以有可能卡住；但是 broadcast 是可以的

写一个正确且高效的 gpu code：

- 管理 gpu cpu data transfer
- parameter tunning（block size 的搜索，for 循环搜索block size就可以，gpu循环很快）
- 避免 atomic ops
- pytorch 很快
- software pipelining 软流水，对于 sqrt、cos、sin 会快很多

example

~~~c++
void compute() {
  for (int i = 0; i < ARRSZ; ++i) {
    ++bin[arr[i]];
  }
}// cpu 350ms

void compute() {
  #pragma omp parallel for num_threads(40)
  for (int i = 0; i < ARRSZ; ++i) {
    int c = arr[i];
  #pragma omp atomic
    ++bin[c];
  }
}// 6s，++bin[c]是串行指令，还要加上 atomic

void compute() {
#pragma omp parallel for num_threads(40)
  for (int t = 0; t < 40; ++t) {
    int tmp_bin[256] = {}
    for (int i = t; i < ARRSZ; i += 40) {
      int c = arr[i];
      ++tmp_bin[c];
    }
    for (int j = 0; j < 256; ++j) {
      int a = tmp_bin[j];
    #pragma omp atomic
      bin[j] += a;
    }
  }
} // 200ms, each thread processes elements with a fixed step

// add locality:
void compute() {
  int blk = (ARRSZ + 39) / 40;
#pragma omp parallel for num_threads(40)
  for (int t = 0; t < 40; ++t) {
    imt tmp_bin[256] = {};
    int l = blk * t, r = blk * (t + 1);
    if (r > ARRSZ)
      r = ARRSZ;
   	for (int i = l; i < r; ++i) {
      int c = arr[i];
      ++tmp_bin[c];
    }
    for (int j = 0; j < 256; ++j) {
      int a = tmp_bin[j];
    #pragma omp atomic
      bin[j] += a;
    }
  }
} // 45 ms

~~~



~~~c++
__global__ void histogram_kernel_v1(unsigned char *array, unsigned int *bins) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  atomicAdd(&bins[array[tid], 1u]);
}

void compute() {
  const int block_size = 256;
  histogram_kernel_v1<<<ARRSZ / block_size, block_size>>>(arr_gpu, bin_gpu);
  assert(cudaSuccess == cudaDevicesychronize());
} // 210 ms
// atomic problem

  
~~~

~~~c++
__shared__ unsigned int bins_shared[256];

__global__ void histogram_kernel_v3(unsigned int *array, unsigned int *bins) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  binx_shared[threadIdx.x] = 0;// block size assumed to be just 256
  __syncthreads();
  
  unsigned int value_u32 = array[tid];
  atomicAdd(&bins_shared[value_u32 & 0x000000FF], 1u);
  atomicAdd(&bins_shared[(value_u32 & 0x0000FF00) >> 8], 1u);
  atomicAdd(&bins_shared[(value_u32 & 0x00FF0000) >> 16], 1u);
  atomicAdd(&bins_shared[(value_u32 & 0xFF000000) >> 24], 1u);
  __syncthreads();
  
  atomicAdd(&bins[threadIdx.x], bins_shared[threadIdx.x]);
}

void compute() {
  const int block_size = 256; // do not change this
  histogram_kernel_v3<<<ARRSZ / block_size / 4, block_size>>>((unsigned int*)arr_gpu, bin_gpu);
  assert(cudaSuccess == cudaDeviceSynchrnoize());
} // 9ms

// Fat kernel, 让kernel干更多的核
~~~

Debug gpu program，cuda-gdb，nvcc -g -G，也可以加断点















